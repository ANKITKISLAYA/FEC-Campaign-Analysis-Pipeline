# FEC Campaign Analysis Pipeline

A modular, Spark-based data engineering pipeline to analyze campaign finance data from the [Federal Election Commission (FEC)](https://www.fec.gov/data/browse-data/?tab=bulk-data). This project demonstrates end-to-end capabilities in data ingestion, processing, aggregation, and visualization using PySpark, ideal for large-scale financial datasets.

---

## Project Structure


FEC-Campaign-Analysis-Pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml               # Main configuration file

â”œâ”€â”€ logs/                                  # Logs generated on pipeline run
â”‚   â”œâ”€â”€ aggregation.log
â”‚   â”œâ”€â”€ ingestion.log
â”‚   â”œâ”€â”€ pipeline_run.log
â”‚   â”œâ”€â”€ processing.log
â”‚   â””â”€â”€ visualization.log

â”œâ”€â”€ notebooks/                             # Jupyter notebooks for EDA and dev
â”‚   â”œâ”€â”€ 1.DataIngestionAndExploration.ipynb
â”‚   â”œâ”€â”€ 2.DataCleaningAndTransformation.ipynb
â”‚   â”œâ”€â”€ 3.DataIntegrationAndAggregation.ipynb
â”‚   â””â”€â”€ 4.DataVisualization.ipynb

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline_run.py                    # Master pipeline runner (all-in-one)

â”‚   â”œâ”€â”€ ingestion_exploration/             # Ingestion-related modules
â”‚   â”‚   â”œâ”€â”€ explore.py
â”‚   â”‚   â”œâ”€â”€ gcs_ingestion.py
â”‚   â”‚   â”œâ”€â”€ ingestion_run.py
â”‚   â”‚   â”œâ”€â”€ mysql_ingestion.py
â”‚   â”‚   â””â”€â”€ save_data.py

â”‚   â”œâ”€â”€ processing/                        # Cleaned-up processing modules
â”‚   â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ allcand_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tranonecomtoano_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cancomlink_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conbyind_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ confromcomtotoanind_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ housencurcam_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ opex_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pacsum_clean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ candmast_clean.py
â”‚   â”‚   â”‚   â””â”€â”€ commmast_clean.py
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ typecasting/
â”‚   â”‚   â”‚   â”œâ”€â”€ allcand_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tranonecomtoano_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cancomlink_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conbyind_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ confromcomtotoanind_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ housencurcam_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ opex_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pacsum_typecast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ candmast_typecast.py
â”‚   â”‚   â”‚   â””â”€â”€ commmast_typecast.py
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”‚   â”œâ”€â”€ allcand_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tranonecomtoano_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cancomlink_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conbyind_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ confromcomtotoanind_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ housencurcam_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ opex_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pacsum_features.py
â”‚   â”‚   â”‚   â”œâ”€â”€ candmast_features.py
â”‚   â”‚   â”‚   â””â”€â”€ commmast_features.py
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ null_counts.py
â”‚   â”‚   â”‚   â””â”€â”€ write_parquet.py
â”‚   â”‚
â”‚   â”‚   â””â”€â”€ processing_run.py              # Main runner for processing pipeline

â”‚   â”œâ”€â”€ aggregation/                       # Data integration & aggregation
â”‚   â”‚   â”œâ”€â”€ aggregate_donor.py
â”‚   â”‚   â”œâ”€â”€ aggregation_run.py
â”‚   â”‚   â”œâ”€â”€ donor_pattern.py
â”‚   â”‚   â”œâ”€â”€ individual_contribution.py
â”‚   â”‚   â”œâ”€â”€ save_data.py
â”‚   â”‚   â””â”€â”€ spending_pattern.py

â”‚   â”œâ”€â”€ visualization/                     # All visual analytics code
â”‚   â”‚   â”œâ”€â”€ plots/                         # Folder where plots are saved
â”‚   â”‚   â”œâ”€â”€ donationanalysis_plot.py
â”‚   â”‚   â”œâ”€â”€ donorsanalysis_plot.py
â”‚   â”‚   â”œâ”€â”€ individualcontributions_plot.py
â”‚   â”‚   â”œâ”€â”€ totalexpenbymonth_plot.py
â”‚   â”‚   â””â”€â”€ visualization_run.py

â”‚   â””â”€â”€ utils/                             # Reusable utility functions
â”‚       â”œâ”€â”€ load_config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ spark_session.py

â””â”€â”€ README.md                              # Documentation on setup and usage



---

## Technologies Used

- **Apache Spark** (PySpark)
- **Google Cloud Storage (GCS)** & **MySQL**
- **Matplotlib & Seaborn** for visualizations
- **Modular Python Packages** for scalability
- **YAML** for dynamic pipeline configuration
- **Logging** for debuggable pipeline stages

---

## Pipeline Stages

| Stage            | Description |
|------------------|-------------|
| ðŸ”½ Ingestion      | Ingests raw datasets from GCS & MySQL |
| ðŸ§¹ Processing     | Cleans, typecasts, engineers features |
| ðŸ“Š Aggregation    | Aggregates donor & spending patterns |
| ðŸ“ˆ Visualization  | Creates insightful financial plots |
| ðŸ§© Integration    | Controlled by `pipeline_run.py` |

---

## How to Run

1. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
